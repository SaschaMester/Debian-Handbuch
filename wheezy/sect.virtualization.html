<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>12.2. Virtualisierung</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css" /><link rel="stylesheet" media="print" href="Common_Content/css/print.css" type="text/css" /><meta name="generator" content="publican 3.1.5" /><meta name="package" content="Debian-debian-handbook-7-de-DE-1.0-1" /><meta name="keywords" content="RAID, LVM, FAI, Voreinstellung, Überwachung, Virtualisierung, Xen, LXC" /><link rel="home" href="index.html" title="Das Debian Administrationshandbuch" /><link rel="up" href="advanced-administration.html" title="Kapitel 12. Erweiterte Verwaltung" /><link rel="prev" href="advanced-administration.html" title="Kapitel 12. Erweiterte Verwaltung" /><link rel="next" href="sect.automated-installation.html" title="12.3. Automatische Installation" /></head><body><p id="title"><a class="left" href="http://www.debian.org"><img alt="Product Site" src="Common_Content/images//image_left.png" /></a><a class="right" href="http://debian-handbook.info"><img alt="Documentation Site" src="Common_Content/images//image_right.png" /></a></p><ul class="docnav top"><li class="previous"><a accesskey="p" href="advanced-administration.html"><strong>Zurück</strong></a></li><li class="home">Das Debian Administrationshandbuch</li><li class="next"><a accesskey="n" href="sect.automated-installation.html"><strong>Weiter</strong></a></li></ul><div class="section"><div class="titlepage"><div><div><h2 class="title"><a xmlns="" id="sect.virtualization"></a>12.2. Virtualisierung</h2></div></div></div><div class="para">
			Virtualisierung <a id="idm139785309845824" class="indexterm"></a> ist einer der größten Fortschritte der letzten Jahre im Computerwesen. Der Ausdruck umfasst verschiedene Abstraktionen und Techniken der Simulation virtueller Rechner mit unterschiedlichen Graden der Unabhängigkeit von der tatsächlichen Hardware. Dabei kann ein physischer Server mehrere Systeme beherbergen, die gleichzeitig und getrennt voneinander funktionieren. Es gibt zahlreiche Anwendungen, und sie rühren häufig von dieser Trennung her: zum Beispiel Testumgebungen mit unterschiedlichen Konfigurationen, oder die getrennte Unterbringung von Diensten auf unterschiedlichen virtuellen Rechnern aus Sicherheitsgründen.
		</div><div class="para">
			Es gibt zahlreiche Virtualisierungslösungen, jede mit ihren eigenen Vor- und Nachteilen. Dieses Buch konzentriert sich auf Xen, LXC und KVM, es gibt jedoch weitere bemerkenswerte Umsetzungen einschließlich der folgenden:
		</div><a id="idm139785309843712" class="indexterm"></a><a id="idm139785309842592" class="indexterm"></a><a id="idm139785309841472" class="indexterm"></a><a id="idm139785309840352" class="indexterm"></a><a id="idm139785311778688" class="indexterm"></a><a id="idm139785311777568" class="indexterm"></a><div class="itemizedlist"><ul><li class="listitem"><div class="para">
					QEMU ist ein Software-Emulator eines vollständigen Rechners. Die Leistungen sind weit geringer als die Geschwindigkeit, die man mit einer tatsächlichen Ausführung erreichen könnte, aber mit ihm ist es möglich, nicht modifizierte oder experimentelle Betriebssysteme auf der emulierten Hardware auszuführen. Mit ihm kann man auch eine andere Hardware-Architektur emulieren: so kann zum Beispiel ein <span class="emphasis"><em>amd64</em></span>-System einen <span class="emphasis"><em>arm</em></span>-Rechner emulieren. QEMU ist freie Software. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li class="listitem"><div class="para">
					Bochs ist eine weitere freie virtuelle Maschine, die jedoch nur die x86-Architektur emuliert (i386 and amd64)t.
				</div></li><li class="listitem"><div class="para">
					VMware ist ein proprietärer virtueller Rechner; da er einer der ältesten ist, ist er auch einer der bekanntesten. Er funktioniert nach ähnlichen Prinzipien wie QEMU. VMware bietet erweiterte Funktionen wie Schnappschüsse eines laufenden virtuellen Rechners. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li class="listitem"><div class="para">
					VirtualBox ist eine virtuelle Maschine, die überwiegend aus freier Software besteht (wobei jedoch einige zusätzliche Komponenten unter einer proprietären Lizenz stehen). Sie ist jünger als VMware und auf die i386- und amd64-Architekturen beschränkt, aber sie enthält einige Snapshot- und andere Features. VirtualBox ist seit <span class="distribution distribution">Lenny</span> in Debian enthalten. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div class="para">
				Xen <a id="idm139785311765984" class="indexterm"></a> ist eine Lösung zur „Paravirtualisierung“. Es führt zwischen der Hardware und den darüber liegenden Systemen eine dünne Abstraktionsschicht ein, die „Hypervisor“ genannt wird. Diese agiert als Schiedsrichter, der den Zugang der virtuellen Rechner zur Hardware kontrolliert. Er wickelt jedoch nur einige der Instruktionen ab, der Rest wird direkt von der Hardware im Auftrag des Systems ausgeführt. Der Hauptvorteil liegt darin, dass die Leistung nicht abnimmt und die Systeme so fast dieselbe Geschwindigkeit wie bei direkter Ausführung erreichen. Die Kehrseite besteht darin, dass die Kernel der Betriebssysteme, die man mit einem Xen-Hypervisor verwenden möchte, angepasst werden müssen, um mit Xen zu funktionieren.
			</div><div class="para">
				Lassen Sie uns einige Zeit bei den Ausdrücken bleiben. Der Hypervisor ist die unterste Schicht, die direkt auf der Hardware läuft, sogar unterhalb des Kernels. Dieser Hypervisor kann die übrige Software auf verschiedene <span class="emphasis"><em>Domains</em></span> aufteilen, die man als ebenso viele virtuelle Rechner ansehen kann. Eine dieser Domains (die erste, die gestartet wird) wird als <span class="emphasis"><em>dom0</em></span> bezeichnet und spielt eine besondere Rolle, da nur diese Domain den Hypervisor und die Ausführung der übrigen Domains kontrollieren kann. Diese übrigen Domains werden <span class="emphasis"><em>domU</em></span> genannt. Mit anderen Worten und aus der Sicht des Benutzers entspricht <span class="emphasis"><em>dom0</em></span> dem „Host“ bei anderen Virtualisierungssystemen, während eine <span class="emphasis"><em>domU</em></span> als „Gast“ angesehen werden kann.
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>KULTUR</em></span> Xen und die verschiedenen Linux-Versionen</strong></p></div></div></div><div class="para">
				Xen ist ursprünglich als Satz von Patches entwickelt worden, die außerhalb der offiziellen Baumstruktur standen und nicht mit dem Linux-Kernel integriert waren. Zur gleichen Zeit benötigten mehrere aufkommenden Virtualisierungssysteme (einschließlich KVM) einige allgemeine virtualisierungsbezogene Funktionen zur Erleichterung ihrer Integration, und der Linux-Kernel bekam diesen Satz von Funktionen (als <span class="emphasis"><em>paravirt_ops</em></span>- oder <span class="emphasis"><em>pv_ops</em></span>-Schnittstelle bekannt). Da die Xen-Patches einige Funktionsweisen dieser Schnittstelle duplizierten, konnten sie nicht offiziell akzeptiert werden.
			</div><div class="para">
				Xensource, das Unternehmen, das hinter Xen steht, musste daher Xen auf dieses neue System portieren, so dass die Xen-Patches mit dem offiziellen Linux-Kernel zusammengeführt werden konnten. Dies bedeutete, dass eine Menge Code umgeschrieben werden musste, und obwohl Xensource bald eine funktionierende Version hatte, die auf der paravirt_ops-Schnittstelle basierte, wurden die Patches nur schrittweise mit dem offiziellen Kernel zusammengeführt. Die Zusammenführung war mit Linux 3.0 abgeschlossen. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div class="para">
				Da <span class="distribution distribution">Wheezy</span> auf der Version 3.2 des Linux-Kernels basiert, enthalten die Standardpakete <span class="pkg pkg">linux-image-686-pae</span> und <span class="pkg pkg">linux-image-amd64</span> bereits die nötigen Programmzeilen und die distributionsspezifischen Patches, die für <span class="distribution distribution">Squeeze</span> und frühere Versionen von Debian noch erforderlich waren, werden nicht mehr benötigt. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div class="para">
				Zur Verwendung von Xen unter Debian sind drei Komponenten erforderlich:
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>HINWEIS</em></span> Mit Xen kompatible Architekturen</strong></p></div></div></div><div class="para">
				Xen gibt es zur Zeit nur für die i386- und amd64-Architekturen. Zudem benutzt es Prozessorinstruktionen, die nicht immer in allen Rechnern der i386-Klasse unterstützt worden sind. Man beachte, dass die meisten Prozessoren der Pentiumklasse (oder besser), die nach 2001 hergestellt worden sind, funktionieren werden. Die Einschränkung gilt daher nur in wenigen Situationen.
			</div></div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>KULTUR</em></span> Xen und Nicht-Linux-Kernel</strong></p></div></div></div><div class="para">
				Xen erfordert Anpassungen bei allen Betriebssystemen, die man darauf laufen lassen möchte; nicht alle Kernel haben in dieser Hinsicht den gleichen Grad an Reife. Viele sind voll funktionsfähig, sowohl als dom0 als auch als domU: Linux 3.0 und später, NetBSD 4.0 und später sowie OpenSolaris. Andere, wie zum Beispiel OpenBSD 4.0, FreeBSD 8 und Plan 9 funktionieren nur als domU.
			</div><div class="para">
				Wenn Xen sich jedoch auf die speziell für eine Virtualisierung vorgesehenen Hardwarefunktionen (die es nur bei neueren Prozessoren gibt) stützen kann, können selbst nicht modifizierte Betriebssysteme (einschließlich Windows) als domU laufen.
			</div></div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						Der Hypervisor an sich. Je nach verfügbarer Hardware ist das entsprechende Paket entweder <span class="pkg pkg">xen-hypervisor-4.01-i386</span> oder <span class="pkg pkg">xen-hypervisor-4.1-amd64</span>.
					</div></li><li class="listitem"><div class="para">
						Ein Kernel, der auf diesem Hypervisor läuft. Jeder jüngere Kernel als 3.0 läuft, einschließlich der Version 3.2 in <span class="distribution distribution">Wheezy</span>.
					</div></li><li class="listitem"><div class="para">
						Die i386-Architektur erfordert zudem eine Standardbibliothek mit passenden Patches, um Xen nutzen zu können; diese befindet sich im Paket <span class="pkg pkg">libc6-xen</span>.
					</div></li></ul></div><div class="para">
				Um den Aufwand der manuellen Auswahl dieser Komponenten zu vermeiden, stehen komfortablerweise einige fertige Pakete bereit (wie zum Beispiel <span class="pkg pkg">xen-linux-system-686-pae</span> und <span class="pkg pkg">xen-linux-system-amd64</span>); sie alle installieren eine als gut bekannte Kombination aus passendem Hypervisor und Kernelpaketen. Der Hypervisor bringt zudem das Paket <span class="pkg pkg">xen-utils-4.1</span> mit sich, das Hilfsprogramme zur Steuerung des Hypervisors von dom0 aus enthält. Dieses wiederum bringt die passende Standardbibliothek mit sich. Während der Installation all dieser Komponenten erstellen Konfigurationsskripten außerdem einen neuen Eintrag im Menü des Grub Boot-Loaders, mit dem der ausgewählte Kernel in einer Xen dom0 gestartet wird. Man beachte jedoch, dass dieser Eintrag gewöhnlich nicht zuoberst in der Liste steht und deshalb nicht standardmäßig ausgewählt wird. Falls dies nicht das erwünschte Verhalten ist, kann es mit folgenden Befehlen verändert werden:
			</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code class="computeroutput"># </code><strong class="userinput"><code>update-grub
</code></strong></pre><div class="para">
				Nachdem diese Voraussetzungen installiert sind, besteht der nächste Schritt darin, das Verhalten von dom0 selbst zu testen; hierzu gehört ein Neustart des Hypervisors und des Xen-Kernels. Das System sollte auf normale Art hochfahren mit einigen zusätzlichen Meldungen auf dem Terminal während der frühen Initialisierungsschritte.
			</div><div class="para">
				Jetzt ist es an der Zeit, unter Verwendung der Hilfsprogramme aus <span class="pkg pkg">xen-tools</span> tatsächlich brauchbare Systeme auf dem domU-System zu installieren. Dieses Paket stellt den Befehl <code class="command">xen-create-image</code> bereit, der die Aufgabe weitgehend automatisiert. Der einzig zwingend notwendige Parameter ist <code class="literal">--hostname</code>, der domU einen Namen gibt. Andere Optionen sind zwar ebenfalls wichtig, können aber in der Konfigurationsdatei <code class="filename">/etc/xen-tools/xen-tools.conf</code> gespeichert werden, und ihr Fehlen in der Befehlszeile führt nicht zu einer Fehlermeldung. Es ist daher wichtig, entweder vor der Erstellung von Abbildern den Inhalt dieser Datei zu überprüfen oder beim Aufruf des Befehls <code class="command">xen-create-image</code> zusätzliche Parameter zu verwenden. Zu den wichtigen und beachtenswerten Parametern gehören folgende:
			</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						<code class="literal">--memory</code>, um den Umfang an RAM festzulegen, den das neu erstellte System nutzen kann;
					</div></li><li class="listitem"><div class="para">
						<code class="literal">--size</code> und <code class="literal">--swap</code>, um die Größe der „virtuellen Platten“ zu definieren, die der domU zur Verfügung stehen;
					</div></li><li class="listitem"><div class="para">
						<code class="literal">--debootstrap</code>, um zu veranlassen, dass das neue System mit <code class="command">debootstrap</code> installiert wird; in diesem Fall wird meistens auch die Option <code class="literal">--dist</code> verwendet (mit dem Namen einer Distribution wie zum Beispiel <span class="distribution distribution">wheezy</span>).
					</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>WEITERE SCHRITTE</em></span> Ein Nicht-Debian-System in einer domU installieren</strong></p></div></div></div><div class="para">
						Im Falle eines Nicht-Linux-Systems sollte man darauf achten, den Kernel, den die domU verwenden soll, mit der Option <code class="literal">--kernel</code> zu bestimmen.
					</div></div></li><li class="listitem"><div class="para">
						<code class="literal">--dhcp</code> legt fest, dass die Netzwerkkonfiguration der domU durch DHCP besorgt wird, während <code class="literal">--ip</code> die Benennung einer statischen IP-Adresse ermöglicht.
					</div></li><li class="listitem"><div class="para">
						Schließlich muss noch eine Speichermethode für die zu erstellenden Abbilder (diejenigen, die von der domU aus als Festplatten gesehen werden) gewählt werden. Die einfachste Methode besteht darin, mit der Option <code class="literal">--dir</code> auf der dom0 eine Datei für jedes Gerät zu erstellen, das der domU zur Verfügung stehen soll. Für Systeme, die LVM verwenden, besteht die Alternative darin, die Option <code class="literal">--lvm</code> zu nutzen, gefolgt von dem Namen einer Volume-Gruppe; <code class="command">xen-create-image</code> erstellt dann ein neues logisches Volume innerhalb dieser Gruppe, und dieses logische Volume wird der domU als Festplatte zur Verfügung gestellt.
					</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>HINWEIS</em></span> Speicherung in der domU</strong></p></div></div></div><div class="para">
						Ganze Festplatten können ebenso in die domU exportiert werden wie auch Partitionen, RAID-Anordnungen oder bereits in LVM bestehende logische Volumes. Diese Vorgänge werden jedoch nicht durch <code class="command">xen-create-image</code> automatisiert. Es ist daher sinnvoll, die Konfigurationsdatei des Xen-Abbildes zu editieren, nachdem sie mit dem Befehl <code class="command">xen-create-image</code> erstmals erstellt worden ist.
					</div></div></li></ul></div><div class="para">
				Nachdem diese Entscheidungen getroffen sind, können wir das Abbild der zukünftigen Xen-domU erstellen:
			</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=wheezy --role=udev</code></strong>
<code class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  wheezy
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.2.0-4-686-pae
Initrd path    :  /boot/initrd.img-3.2.0-4-686-pae
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  wheezy
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  48su67EW
</code></pre><div class="para">
				Wir haben jetzt einen virtuellen Rechner, er läuft zur Zeit jedoch nicht (und belegt daher lediglich Platz auf der Festplatte der dom0). Wir können selbstverständlich weitere Abbilder erstellen, möglicherweise mit anderen Parametern.
			</div><div class="para">
				Bevor wir diese virtuellen Rechner starten, müssen wir festlegen, wie wir auf sie zugreifen werden. Sie können natürlich als eigenständige Rechner angesehen werden, auf die nur über ihre jeweilige Systemkonsole zugegriffen wird, dies entspricht jedoch nur selten dem Nutzungsmuster. Meistens wird eine domU als entfernter Server angesehen, auf den nur über ein Netzwerk zugegriffen wird. Es wäre jedoch ziemlich umständlich, für jede domU eine Netzwerkkarte hinzuzufügen. Deshalb ist es möglich, mit Xen virtuelle Schnittstellen zu erstellen, die von jeder Domain gesehen und auf übliche Weise benutzt werden können. Man beachte, dass diese Karten, obwohl sie virtuell sind, nur von Nutzen sind, wenn sie mit einem Netzwerk verbunden sind, selbst wenn dieses virtuell ist. Xen bietet zu diesem Zweck mehrere Netzwerkmodelle:
			</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						Das einfachste Modell ist das <span class="emphasis"><em>bridge</em></span>-Modell; alle eth0-Netzwerkkarten (sowohl in der dom0 als auch in den domU-Systemen) verhalten sich so, als wären sie direkt an einen Ethernet-Switch angeschlossen.
					</div></li><li class="listitem"><div class="para">
						Dann kommt das <span class="emphasis"><em>routing</em></span>-Modell, bei dem dom0 als Router agiert, der zwischen den domU-Systemen und dem (physischen) externen Netzwerk steht.
					</div></li><li class="listitem"><div class="para">
						Schließlich befindet sich im <span class="emphasis"><em>NAT</em></span>-Modell die dom0 ebenfalls zwischen den domU-Systemen und dem übrigen Netzwerk, jedoch sind die domU-Systeme von außen nicht direkt zugänglich, sondern der Datenverkehr wird auf der dom0 einer „Network Address Translation“ unterworfen.
					</div></li></ul></div><div class="para">
				Zu diesen drei Netzknoten gehören eine Reihe von Schnittstellen mit ungewöhnlichen Bezeichnungen, wie zum Beispiel <code class="filename">vif*</code>, <code class="filename">veth*</code>, <code class="filename">peth*</code> und <code class="filename">xenbr0</code>. Der Xen-Hypervisor ordnet sie gemäß dem an, was auch immer als Layout festgelegt worden ist, unter der Kontrolle der Hilfsprogramme auf der Anwenderebene. Da die NAT- und Routing-Modelle besonderen Fällen vorbehalten sind, beschäftigen wir uns hier nur mit dem Bridging-Modell.
			</div><div class="para">
				Die Standardkonfiguration der Xen-Pakete verändert die systemweite Netzwerk-Konfiguration nicht. Jedoch ist der <code class="command">xend</code>-Daemon so konfiguriert, dass er virtuelle Netzwerkschnittstellen in eine bereits bestehende Netzwerkbrücke integriert (wobei <code class="filename">xenbr0</code> Vorrang erhält, falls es mehrere solcher Brücken gibt). Wir müssen daher eine Brücke in <code class="filename">/etc/network/interfaces</code> einrichten (wozu das Paket <span class="pkg pkg">bridge-utils</span> installiert werden muss, weshalb es vom Paket <span class="pkg pkg">xen-utils-4.1</span> empfohlen wird), um den bestehenden eth0-Eintrag zu ersetzen:
			</div><pre class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div class="para">
				Nach einem Neustart, um sicherzustellen, dass die Brücke automatisch erstellt wird, können wir jetzt die domU mit den Xen-Steuerprogrammen starten, insbesondere mit dem Befehl <code class="command">xm</code>. Mit diesem Befehl ist es möglich, Verschiedenes mit den Domains zu machen, unter anderem sie aufzulisten und sie zu starten oder zu beenden.
			</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xm list</code></strong>
<code class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong class="userinput"><code>xm create testxen.cfg</code></strong>
<code class="computeroutput">Using config file "/etc/xen/testxen.cfg".
Started domain testxen (id=1)
# </code><strong class="userinput"><code>xm list</code></strong>
<code class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>VORSICHT</em></span> Nur eine domU je Abbild!</strong></p></div></div></div><div class="para">
				Während mehrere domU-Systeme natürlich gleichzeitig laufen können, muss jedes von ihnen sein eigenes Abbild verwenden, da jede domU den Eindruck erhält, dass sie auf ihrer eigenen Hardware läuft (abgesehen von dem kleinen Kernelanteil, der mit dem Hypervisor kommuniziert). Vor allem ist es nicht möglich, dass zwei domU-Systeme zur selben Zeit Speicherplatz gemeinsam benutzen. Falls die domU-Systeme nicht zur selben Zeit laufen, können sie jedoch eine einzige Auslagerungspartition oder die Partition, die das Dateisystem <code class="filename">/home</code> enthält, wiederverwenden.
			</div></div><div class="para">
				Man beachte, dass die domU <code class="filename">testxen</code> wirklichen Speicher des RAM verwendet, der ansonsten für die dom0 verfügbar wäre, und keinen simulierten Speicher. Man sollte daher darauf achten, das physische RAM entsprechend zuzuteilen, wenn man einen Server einrichtet, auf dem Xen-Instanzen laufen sollen.
			</div><div class="para">
				Voilà! Unser virtueller Rechner startet. Wir können auf ihn auf zweifache Art zugreifen. Der normale Weg besteht darin, sich mit ihm „aus der Ferne“ über das Netzwerk zu verbinden, wie wir es auch bei einem wirklichen Rechner tun würden; hierzu ist es normalerweise erforderlich, entweder einen DHCP-Server oder eine DNS-Konfiguration einzurichten. Der andere Weg, der der einzige sein könnte, falsch die Netzwerk-Konfiguration nicht korrekt war, besteht darin, über den Befehl <code class="command">xm console</code> die Konsole <code class="filename">hvc0</code> zu benutzen:
			</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xm console testxen</code></strong>
<code class="computeroutput">[...]

Debian GNU/Linux 7.0 testxen hvc0

testxen login: </code></pre><div class="para">
				Man kann dann eine Sitzung öffnen, als säße man an der Tastatur des virtuellen Rechners. Zur Trennung von dieser Konsole dient die Tastenkombination <span class="keycap"><strong>Strg</strong></span>+<span class="keycap"><strong>]</strong></span>.
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TIPP</em></span> Direkt zur Konsole gelangen</strong></p></div></div></div><div class="para">
				Manchmal möchte man ein domU-System starten und direkt zu seiner Konsole gelangen; deshalb kann dem Befehl <code class="command">xm create</code> der Schalter <code class="literal">-c</code> übergeben werden. Wenn man eine domU mit diesem Schalter startet, werden während des Hochfahrens des Systems alle Meldungen angezeigt.
			</div></div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div><div class="para">
				OpenXenManager (im Paket <span class="pkg pkg">openxenmanager</span>) ist ein grafisches Interface das entfernte Verwaltung von Xen Domänen über das API von Xen ermöglicht. Es kann so Xen Domänen aus der Ferne kontrollieren. Es stellt den Großteil der Funktionalität des Befehls <code class="command">xm</code> zur Verfügung.
			</div></div><div class="para">
				Sobald eine domU läuft, kann sie so wie jeder andere Server verwendet werden (da sie schließlich ein GNU/Linux-System ist). Ihr Status als virtueller Rechner ermöglicht jedoch einige zusätzliche Funktionen. So kann eine domU zum Beispiel mit den Befehlen <code class="command">xm pause</code> und <code class="command">xm unpause</code> vorübergehend angehalten und dann wieder fortgesetzt werden. Man beachte, dass bei einer angehaltenen domU, obwohl sie keine Prozessorleistung in Anspruch nimmt, der ihr zugeordnete Speicherplatz weiterhin belegt ist. Es könnte interessant sein, hier die Befehle <code class="command">xm save</code> und <code class="command">xm restore</code> in Erwägung zu ziehen: das Speichern einer domU gibt die Ressourcen frei, die vorher von dieser domU verwendet wurden, einschließlich des RAM. Wenn sie fortgesetzt wird (oder eigentlich ihr Pausieren beendet wird), bemerkt eine domU außer dem Fortschreiten der Zeit nichts. Falls eine domU läuft, wenn die dom0 heruntergefahren wird, speichern die gebündelten Skripten automatisch die domU, und stellen sie beim nächsten Hochfahren wieder her. Dies schließt natürlich die gleichen Unannehmlichkeiten ein, die entstehen, wenn man zum Beispiel einen Laptop in den Ruhezustand versetzt; insbesondere, dass Netzwerkverbindungen verfallen, falls die domU zu lange ausgesetzt ist. Man beachte auch, dass Xen insofern mit einem Großteil der ACPI-Energieverwaltung inkompatibel ist, als es nicht möglich ist, das Host-System (die dom0) in den Bereitschaftsbetrieb zu versetzen.
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>DOKUMENTATION</em></span> <code class="command">xm</code>-Optionen</strong></p></div></div></div><div class="para">
				Die meisten der <code class="command">xm</code>-Unterbefehle erwarten ein oder mehrere Argumente, häufig einen domU-Namen. Diese Argumente sind auf der Handbuchseite <span class="citerefentry"><span class="refentrytitle">xm</span>(1)</span> gut beschrieben.
			</div></div><div class="para">
				Das Anhalten oder Neustarten einer domU kann entweder aus dieser domU heraus geschehen (mit dem Befehl <code class="command">shutdown</code>) oder von der dom0 aus mit <code class="command">xm shutdown</code> oder <code class="command">xm reboot</code>.
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>WEITERE SCHRITTE</em></span> Weitergehendes Xen</strong></p></div></div></div><div class="para">
				Xen verfügt über wesentlich mehr Funktionen als wir in diesen wenigen Absätzen beschreiben können. Vor allem ist das System sehr dynamisch, und viele Parameter einer Domain (wie zum Beispiel der Umfang des zugewiesenen Speichers, die sichtbaren Festplatten, das Verhalten der Aufgabensteuerung und so weiter) können eingestellt werden, selbst wenn die Domain läuft. Eine domU kann sogar auf einen anderen Server verschoben werden, ohne abgeschaltet zu werden und ohne ihre Netzwerkverbindungen zu verlieren! Die Hauptinformationsquelle für alle diese weitergehenden Aspekte ist die offizielle Xen-Dokumentation. <div xmlns="" class="url">→ <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><div class="para">
				Obwohl es dazu benutzt wird, „virtuelle Rechner“ zu erstellen, ist LXC <a id="idm139785311663200" class="indexterm"></a> genaugenommen kein Virtualisierungssystem, sondern ein System, um Gruppen von Prozessen voneinander zu isolieren, obwohl sie alle auf demselben Host laufen. Es macht sich eine Reihe neuerer Entwicklungen im Linux-Kernel zunutze, die gemeinhin als <span class="emphasis"><em>Kontrollgruppen</em></span> bekannt sind, mit denen verschiedene Sätze von Prozessen, die „Gruppen“ genannt werden, bestimmte Aspekte des Gesamtsystems auf unterschiedliche Weise sehen. Dies gilt vor allem für Aspekte wie die Prozesskennungen, die Netzwerkonfiguration und die Einhängepunkte. Eine derartige Gruppe isolierter Prozesse hat keinerlei Zugriff auf die anderen Prozesse des Systems, und ihre Zugriffe auf das Dateisystem können auf einen bestimmten Teilbereich eingegrenzt werden. Sie kann auch ihre eigene Netzwerkschnittstelle und Routing-Tabelle haben, und möglicherweise ist sie so konfiguriert, dass sie nur einen Teil der auf dem System verfügbaren Geräte sieht.
			</div><div class="para">
				Diese Funktionen können kombiniert werden, um eine ganze Prozessfamilie, vom <code class="command">init</code>-Prozess angefangen, zu isolieren, und die sich daraus ergebende Gruppe sieht einem virtuellen Rechner sehr ähnlich. Die offizielle Bezeichnung für eine derartige Anordnung ist ein „Container“ (daher der Name LXC: <span class="emphasis"><em>LinuX Containers</em></span>), jedoch besteht ein wichtiger Unterschied zu einem „wirklichen“ virtuellen Rechner, wie einem der durch Xen oder KVM bereitgestellt wird, darin, dass es keinen zweiten Kernel gibt; der Container verwendet denselben Kernel wie das Host-System. Dies hat Vor- und Nachteile: zu den Vorteilen gehören die exzellente Performance aufgrund fehlender Last durch Overhead, und die Tatsache, dass der Kernel einen vollständigen Überblick über alle Prozesse hat, die auf dem System laufen, wodurch die Steuerung effizienter sein kann, als wenn zwei unabhängige Kernel verschiedene Aufgabensätze steuern würden. Zu den Nachteilen gehört vor allem, dass man in einem Container keinen anderen Kernel laufen lassen kann (sei dies eine andere Linux-Version oder ein völlig anderes Betriebssystem).
			</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>HINWEIS</em></span> Grenzen der LXC-Isolierung</strong></p></div></div></div><div class="para">
				LXC-Container bieten nicht den Grad an Isolierung, der mit schwergewichtigeren Emulatoren oder Virtualisierern erreicht wird. Insbesondere:
			</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						ermöglicht es der Standard-Kernel bei <span class="distribution distribution">Wheezy</span> nicht, den Umfang des Speichers, der einem Container zur Verfügung steht, zu begrenzen; es gibt diese Funktion, und sie kann aktiviert werden, aber sie ist per Voreinstellung deaktiviert, weil sie die Gesamtperformance des Systems belastet; es ist aber einfach, sie mit der Kommandozeilenoption <code class="command">cgroup_enable=memory</code> beim Bootvorgang einzuschalten;
					</div></li><li class="listitem"><div class="para">
						können, da der Kernel vom Host-System und den Containern gemeinsam genutzt wird, in Containern gebundene Prozesse weiterhin auf Kernel-Meldungen zugreifen, wodurch Informationslecks entstehen können, falls Meldungen von einem Container abgegeben werden;
					</div></li><li class="listitem"><div class="para">
						können aus ähnlichen Gründen, falls ein Container beeinträchtigt und eine Kernel-Schwachstelle ausgenutzt wird, die übrigen Container ebenfalls betroffen sein;
					</div></li><li class="listitem"><div class="para">
						überprüft der Kernel Berechtigungen im Dateisystem anhand der numerischen Kennungen für Benutzer und Gruppen; diese Kennungen können je nach Container unterschiedliche Benutzer und Gruppen bezeichnen, woran man denken sollte, falls beschreibbare Teile des Dateisystems von mehreren Containern gemeinsam benutzt werden.
					</div></li></ul></div></div><div class="para">
				Da wir es hier mit einer Isolierung und nicht mit einer einfachen Virtualisierung zu tun haben, ist es schwieriger, einen LXC-Container einzurichten, als nur ein Debian-Installationsprogramm auf einem virtuellen Rechner auszuführen. Wir werden einige Voraussetzungen beschreiben und dann zur Netzwerkkonfigurierung übergehen; damit werden wir in der Lage sein, das System, das in dem Container laufen soll, zu erstellen.
			</div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311649328"></a>12.2.2.1. Vorbereitende Schritte</h4></div></div></div><div class="para">
					Das Paket <span class="pkg pkg">lxc</span> enthält die für die Ausführung von LXC erforderlichen Hilfsprogramme und muss daher installiert werden.
				</div><div class="para">
					LXC benötigt außerdem das Konfigurationssystem für die <span class="emphasis"><em>Kontrollgruppen</em></span>, das ein unter <code class="filename">/sys/fs/cgroup</code> einzuhängendes virtuelles Dateisystem ist. Die Datei <code class="filename">/etc/fstab</code> sollte daher unter anderem folgenden Eintrag enthalten:
				</div><pre class="programlisting scale"># /etc/fstab: static file system information.
[...]
cgroup            /sys/fs/cgroup           cgroup    defaults        0       0
</pre><div class="para">
					<code class="filename">/sys/fs/cgroup</code> wird dann beim Hochfahren automatisch eingehängt; falls kein unmittelbarer Neustart vorgesehen ist, sollte das Dateisystem mit dem Befehl <code class="command">mount /sys/fs/cgroup</code> eingehängt werden.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="sect.lxc.network"></a>12.2.2.2. Netzwerkkonfigurierung</h4></div></div></div><div class="para">
					LXC wird mit dem Ziel installiert, virtuelle Rechner einzurichten; während wir diese natürlich vom Netzwerk getrennt halten und mit ihnen nur über das Dateisystem kommunizieren könnten, ist es in den meisten Anwendungsfällen erforderlich, den Containern wenigstens einen minimalen Netzwerkzugang zu gewähren. Typischerweise erhält jeder Container eine virtuelle Netzwerkschnittstelle, die mit dem wirklichen Netzwerk über eine Bridge verbunden ist. Diese virtuelle Schnittstelle kann entweder direkt an die physische Schnittstelle des Hosts angeschlossen sein (wobei sich der Container dann direkt im Netzwerk befindet) oder an eine weitere virtuelle Schnittstelle, die auf dem Host festgelegt ist (und bei der der Host dann den Datenverkehr filtern oder umleiten kann). In beiden Fällen ist das Paket <span class="pkg pkg">bridge-utils</span> erforderlich.
				</div><div class="para">
					Der einfachste Fall besteht darin, die Datei <code class="filename">/etc/network/interfaces</code> zu editieren, indem die Konfiguration für die physische Schnittstelle (zum Beispiel <code class="literal">eth0</code>) zu einer Bridge-Schnittstelle verschoben (normalerweise <code class="literal">br0</code>) und die Verbindung zwischen ihnen konfiguriert wird. Wenn zum Beispiel die Konfigurationsdatei der Netzwerkschnittstellen Einträge wie die folgenden enthält:
				</div><pre class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div class="para">
					sollten sie deaktiviert und durch folgende ersetzt werden:
				</div><pre class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div class="para">
					Die Auswirkung dieser Konfiguration ähnelt derjenigen, die einträte, falls die Container Rechner wären, die an dasselbe physische Netzwerk angeschlossen sind wie der Host. Die „Bridge“-Konfiguration verwaltet den Übergang der Ethernet-Frames zwischen allen verbundenen Schnittstellen, zu denen sowohl die physische Schnittstelle <code class="literal">eth0</code> als auch die für die Container festgelegten Schnittstellen gehören.
				</div><div class="para">
					In Fällen, in denen diese Konfiguration nicht verwendet werden kann (falls zum Beispiel den Containern keine öffentlichen IP-Adressen zugeordnet werden können), wird eine virtuelle <span class="emphasis"><em>tap</em></span>-Schnittstelle eingerichtet und mit der Bridge verbunden. Die dementsprechende Netzstruktur wird dann zu einer, bei der der Host mit einer zweiten Netzwerkkarte an einen eigenen Switch angeschlossen ist, wobei die Container ebenfalls an diesen Switch angeschlossen sind. Der Host muss in diesem Fall als Gateway für die Container agieren, falls diese mit der Außenwelt kommunizieren sollen.
				</div><div class="para">
					Zusätzlich zu <span class="pkg pkg">bridge-utils</span> ist für diese „üppige“ Konfiguration das Paket <span class="pkg pkg">vde2</span> erforderlich; die Datei <code class="filename">/etc/network/interfaces</code> wird dann zu:
				</div><pre class="programlisting"># Schnittstelle eth0 bleibt unverändert
auto eth0
iface eth0 inet dhcp

# Virtuelle Schnittstelle
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge für Container
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre><div class="para">
					Das Netzwerk kann dann entweder statisch in den Containern eingerichtet werden oder dynamisch mit einem DHCP-Server, der auf dem Host läuft. Solch ein DHCP-Server muss so konfiguriert sein, dass er Anfragen auf der Schnittstelle <code class="literal">br0</code> beantwortet.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311628576"></a>12.2.2.3. Das System einrichten</h4></div></div></div><div class="para">
					Lassen Sie uns jetzt das von dem Container zu verwendende Dateisystem einrichten. Da dieser „virtuelle Rechner“ nicht direkt auf der Hardware laufen wird, sind im Vergleich zu einem Standard-Dateisystem einige Feineinstellungen vorzunehmen, insbesondere was den Kernel, die Geräte und Konsolen betrifft. Glücklicherweise enthält das Paket <span class="pkg pkg">lxc</span> Skripten, die diese Konfigurierung weitestgehend automatisieren. So installieren zum Beispiel die folgenden Befehle (die das Paket <span class="pkg pkg">debootstrap</span> und <span class="pkg pkg">rsync</span> erfordern) einen Debian-Container:
				</div><pre class="screen"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code class="computeroutput">Note: Usually the template option is called with a configuration
file option too, mostly to configure the network.
For more information look at lxc.conf (5)

debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-wheezy-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release
I: Retrieving Release.gpg
[...]
Root password is 'root', please change !
'debian' template installed
'testlxc' created
root@mirwiz:~# </code>
</pre><div class="para">
					Man beachte, dass das Dateisystem zunächst in <code class="filename">/var/cache/lxc</code> erstellt und dann in sein Zielverzeichnis verschoben wird. So lassen sich mehrere identische Container wesentlich schneller erstellen, da sie nur kopiert werden müssen.
				</div><div class="para">
					Man beachte, dass das Skript zum Erstellen des Debian Beispiels eine Option <code class="option">--arch</code> akzeptiert, um die Architaktur anzugeben, die Installiert werden soll, sowie eine Option <code class="option">--release</code>, wenn Sie etwas anderes als das aktuelle "stable" Release von Debian installieren wollen. Sie können auch die Umgebungsvariable <code class="literal">MIRROR</code> auf einen lokalen Debain Spiegel zeigen lassen.
				</div><div class="para">
					Das neu erstellte Dateisystem enthält jetzt ein minimales Debian-System, und per Voreinstellung teilt der Container die Netzwerkschnittstelle mit dem Wirtssystem. Da wir das nicht wollen, editieren wir die Konfigurationsdatei des Containers (<code class="filename">/var/lib/lxc/testlxc/config</code>) und fügen einige <code class="literal">lxc.network.*</code>-Einträge hinzu:
				</div><pre class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</pre><div class="para">
					Diese Einträge bedeuten jeweils, dass eine virtuelle Schnittstelle in dem Container erzeugt wird; dass sie automatisch in Gang gesetzt wird, wenn der besagte Container startet; dass sie automatisch mit der <code class="literal">br0</code>-Bridge auf dem Host verbunden wird; und dass ihre MAC-Adresse wie angegeben lautet. Falls diese letzte Angabe fehlt oder deaktiviert ist, wird eine zufällige MAC-Adresse erzeugt.
				</div><div class="para">
					Ein anderer nützlicher Eintrag in dieser Datei ist die Angabe des Hostnamens:
				</div><pre class="programlisting">lxc.utsname = testlxc
</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311614784"></a>12.2.2.4. Den Container starten</h4></div></div></div><div class="para">
					Nun, da das Abbild unseres virtuellen Rechners fertig ist, wollen wir den Container starten:
				</div><pre class="screen scale" width="94"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-console -n testlxc
</code></strong><code class="computeroutput">Debian GNU/Linux 7 testlxc tty1

testlxc login: </code><strong class="userinput"><code>root</code></strong><code class="computeroutput">
Password: 
Linux testlxc 3.2.0-4-amd64 #1 SMP Debian 3.2.46-1+deb7u1 x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong class="userinput"><code>ps auxwf</code></strong>
<code class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0  10644   824 ?        Ss   09:38   0:00 init [3]  
root      1232  0.0  0.2   9956  2392 ?        Ss   09:39   0:00 dhclient -v -pf /run/dhclient.eth0.pid 
root      1379  0.0  0.1  49848  1208 ?        Ss   09:39   0:00 /usr/sbin/sshd
root      1409  0.0  0.0  14572   892 console  Ss+  09:39   0:00 /sbin/getty 38400 console
root      1410  0.0  0.1  52368  1688 tty1     Ss   09:39   0:00 /bin/login --     
root      1414  0.0  0.1  17876  1848 tty1     S    09:42   0:00  \_ -bash
root      1418  0.0  0.1  15300  1096 tty1     R+   09:42   0:00      \_ ps auxf
root      1411  0.0  0.0  14572   892 tty2     Ss+  09:39   0:00 /sbin/getty 38400 tty2 linux
root      1412  0.0  0.0  14572   888 tty3     Ss+  09:39   0:00 /sbin/getty 38400 tty3 linux
root      1413  0.0  0.0  14572   884 tty4     Ss+  09:39   0:00 /sbin/getty 38400 tty4 linux
root@testlxc:~# </code></pre><div class="para">
					Wir befinden uns nun in dem Container; unser Zugriff auf diejenigen Prozesse beschränkt, die vom Container selbst gestartet wurden, und unser Zugriff auf das Dateisystem ist in ähnlicher Weise auf die zugehörige Teilmenge des gesamten Dateisystems (<code class="filename">/var/lib/lxc/testlxc/rootfs</code>) eingeschränkt. Wir können die Konsole mit <span class="keycap"><strong>Control</strong></span>+<span class="keycap"><strong>a</strong></span> <span class="keycap"><strong>q</strong></span> wieder verlassen.
				</div><div class="para">
					Beachten Sie, dass wir den Container beim Aufruf von <code class="command">lxc-start</code> durch die Option <code class="option">--daemon</code> als Hintergrundprozess laufen lassen. Wir können den Container dann mit einem Befehl wie <code class="command">lxc-kill --name=testlxc</code> schließen.
				</div><div class="para">
					Das Paket <span class="pkg pkg">lxc</span> enthält ein Initialisierungsskript, das beim Hochfahren des Hosts automatisch einen oder mehrere Container starten kann; seine Konfigurationsdatei <code class="filename">/etc/default/lxc</code> ist recht einfach. Man beachte, dass die Konfigurationsdateien der Container in <code class="filename">/etc/lxc/auto/</code> gespeichert werden müssen; viele Benutzer bevorzugen möglicherweise symbolische Verknüpfungen, wie sie mit dem Befehl <code class="command">ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</code> erstellt werden können.
				</div><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>WEITERE SCHRITTE</em></span> Massenvirtualisierung</strong></p></div></div></div><div class="para">
					Da LXC ein sehr leichtgewichtiges Isolierungssystem ist, kann es insbesondere für die Bereitstellung zahlreicher virtueller Server eingerichtet werden. Die Netzwerkkonfiguration wird hierbei im Vergleich zu der, die wir oben beschrieben haben, möglicherweise etwas erweitert sein, die „üppige“ Konfiguration unter Verwendung von <code class="literal">tap</code>- und <code class="literal">veth</code>-Schnittstellen sollte in den meisten Fällen hierfür jedoch ausreichend sein.
				</div><div class="para">
					Es könnte auch sinnvoll sein, einen Teil des Dateisystems, wie zum Beispiel die Unterverzeichnisse <code class="filename">/usr</code> und <code class="filename">/lib</code>, gemeinsam zu nutzen, um so die Duplizierung von Software zu vermeiden, die bei mehreren Containern benötigt wird. Dies wird normalerweise durch <code class="literal">lxc.mount.entry</code>-Einträge in der Konfigurationsdatei des Containers erreicht. Ein interessanter Nebeneffekt besteht darin, dass die Prozesse in diesem Fall weniger physischen Speicher benötigen, da der Kernel erkennen kann, dass die Programme gemeinsam benutzt werden. Die zusätzliche Belastung durch einen weiteren Container kann so auf den für seine spezifischen Daten erforderlichen Plattenplatz und einige zusätzliche Prozesse, die der Kernel einplanen und verwalten muss, reduziert werden.
				</div><div class="para">
					Wir haben selbstverständlich nicht alle verfügbaren Optionen beschrieben; ausführlichere Informationen sind auf den Handbuchseiten <span class="citerefentry"><span class="refentrytitle">lxc</span>(7)</span> und <span class="citerefentry"><span class="refentrytitle">lxc.conf</span>(5)</span> sowie auf weiteren Seiten, auf die sie verweisen, enthalten.
				</div></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="idm139785311590704"></a>12.2.3. Virtualisierung mit KVM</h3></div></div></div><a id="idm139785311589904" class="indexterm"></a><div class="para">
				KVM, das <span class="emphasis"><em>Kernel-based Virtual Machine</em></span> bedeutet, ist in erster Linie ein Kernel-Modul, das den größten Teil der Infrastruktur bereitstellt, die von einem Virtualisierungsprogramm benutzt werden kann, ist jedoch selbst kein Virtualisierungsprogramm. Die eigentliche Steuerung der Virtualisierung wird von einer Anwendung auf der Grundlage von QEMU vorgenommen. Wundern Sie sich nicht, dass dieser Abschnitt über <code class="command">qemu-*</code>-Befehle spricht: er handelt dennoch von KVM.
			</div><div class="para">
				Im Gegensatz zu anderen Virtualisierungssystemen war KVM von Anfang an Teil des Linux-Kernels. Seine Entwickler entschieden sich, die für eine Virtualisierung vorgesehenen Prozessor-Befehlssätze (Intel-VT und AMD-V) zu nutzen, wodurch KVM leichtgewichtig, elegant und ressourcenschonend bleibt. Die Kehrseite ist natürlich, dass KVM hauptsächlich auf i386- und amd64-Prozessoren läuft und zwar nur auf denen, die so neu sind, dass sie über diese Befehlssätze verfügen. Sie können sicher sein, dass Sie einen derartigen Prozessor haben, wenn unter den CPU-Flags in der Datei <code class="filename">/proc/cpuinfo</code> „vmx“ oder „svm“ aufgeführt ist.
			</div><div class="para">
				Mit aktiver Unterstützung seiner Entwicklung durch Red Hat scheint KVM auf dem Wege zu sein, zur Referenz für Linux-Virtualisierung zu werden.
			</div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311585072"></a>12.2.3.1. Vorbereitende Schritte</h4></div></div></div><a id="idm139785311584272" class="indexterm"></a><div class="para">
					Im Gegensatz zu Programmen wie VirtualBox enthält KVM selbst keine Benutzerschnittstelle zur Erstellung und Verwaltung virtueller Rechner. Das Paket <span class="pkg pkg">qemu-kvm</span> stellt nur eine ausführbare Datei zum Start eines virtuellen Rechners bereit sowie ein Initialisierungsskript, das die passenden Kernel-Module lädt.
				</div><a id="idm139785311581600" class="indexterm"></a><a id="idm139785311580640" class="indexterm"></a><div class="para">
					Glücklicherweise stellt Red Hat mit der Entwicklung der Bibliothek <span class="emphasis"><em>libvirt</em></span> und der dazugehörigen Werkzeuge des <span class="emphasis"><em>virtual machine manager</em></span> einen weiteren Satz von Hilfsprogrammen zur Lösung dieses Problems bereit. Mit libvirt ist es möglich, virtuelle Rechner einheitlich zu verwalten unabhängig von dem Virtualisierungssystem, das hinter den Kulissen beteiligt ist (gegenwärtig unterstützt es QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare und UML). <code class="command">virtual-manager</code> ist eine grafische Schnittstelle, die libvirt zur Erstellung und Verwaltung virtueller Rechner benutzt.
				</div><a id="idm139785311576960" class="indexterm"></a><div class="para">
					Zunächst installieren wir mit <code class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code> die erforderlichen Pakete. <span class="pkg pkg">libvirt-bin</span> stellt den Daemon <code class="command">libvirtd</code> zur Verfügung, mit dem (unter Umständen aus der Ferne) die Verwaltung der virtuellen Rechner, die auf dem Host laufen, möglich ist, und der die erforderlichen virtuellen Rechner startet, wenn der Host hochfährt. Zusätzlich enthält dieses Paket das Befehlszeilenwerkzeug <code class="command">virsh</code>, das die Steuerung der Rechner ermöglicht, die von <code class="command">libvirtd</code> verwaltet werden.
				</div><div class="para">
					Das Paket <span class="pkg pkg">virtinst</span> stellt den Befehl <code class="command">virt-install</code> bereit, mit dem es möglich ist, virtuelle Rechner von der Befehlszeile aus zu erstellen. Und schließlich ermöglicht <span class="pkg pkg">virt-viewer</span> den Zugriff auf die grafische Konsole eines virtuellen Rechners.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311569296"></a>12.2.3.2. Netzwerkkonfigurierung</h4></div></div></div><div class="para">
					Genauso wie in Xen und LXC gehört zu der häufigsten Netzwerkkonfiguration eine Bridge, mit der die Netzwerkschnittstellen der virtuellen Rechner zusammengefasst werden (siehe <a class="xref" href="sect.virtualization.html#sect.lxc.network">Abschnitt 12.2.2.2, „Netzwerkkonfigurierung“</a>).
				</div><div class="para">
					Stattdessen kann (und das ist die Voreinstellung bei KVM) dem virtuellen Rechner eine private Adresse (im Bereich von 192.168.122.0/24) zugeordnet und NAT eingerichtet werden, so dass der virtuelle Rechner auf das externe Netzwerk zugreifen kann.
				</div><div class="para">
					Für den Rest dieses Abschnitts wird angenommen, dass der Host über eine <code class="literal">eth0</code> als physische Schnittstelle und eine <code class="literal">br0</code>-Bridge verfügt, und das Erstere mit Letzterer verbunden ist.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785311564928"></a>12.2.3.3. Installation mit <code class="command">virt-install</code></h4></div></div></div><a id="idm139785311563808" class="indexterm"></a><div class="para">
					Die Erstellung eines virtuellen Rechners ist der Installation eines normalen Systems sehr ähnlich, außer dass die Eigenschaften des virtuellen Rechners in einer scheinbar endlosen Befehlszeile beschrieben werden.
				</div><div class="para">
					In der Praxis bedeutet dies, dass wir das Debian-Installationsprogramm verwenden, indem wir den virtuellen Rechner auf einem virtuellen DVD-ROM-Laufwerk hochfahren, dem ein auf dem Host-System gespeichertes DVD-Abbild von Debian zugeordnet ist. Der virtuelle Rechner exportiert seine grafische Konsole über das VNC-Protokoll (für Einzelheiten siehe <a class="xref" href="sect.remote-login.html#sect.remote-desktops">Abschnitt 9.2.2, „Entfernte grafische Arbeitsflächen benutzen“</a>), so dass wir den Installationsprozess steuern können.
				</div><div class="para">
					Zunächst müssen wir libvirtd mitteilen, wo die Plattenabbilder gespeichert werden sollen, es sei denn, dass der voreingestellte Ort (<code class="filename">/var/lib/libvirt/images/</code>) in Ordnung ist.
				</div><pre class="screen"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>mkdir /srv/kvm</code></strong>
<code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div class="para">
					Wir wollen jetzt mit dem Installationsprozess für den virtuellen Rechner beginnen und uns die wichtigsten Optionen des Befehls <code class="command">virt-install</code> ansehen. Der Befehl registriert den virtuellen Rechner und seine Parameter in libvirtd und startet ihn dann, so dass seine Installierung fortgesetzt werden kann.
				</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virt-install --connect qemu:///system  <img class="callout" src="Common_Content/images/1.png" alt="1" border="0" id="virtinst.connect" />
               --virt-type kvm           <img class="callout" src="Common_Content/images/2.png" alt="2" border="0" id="virtinst.type" />
               --name testkvm            <img class="callout" src="Common_Content/images/3.png" alt="3" border="0" id="virtinst.name" />
               --ram 1024                <img class="callout" src="Common_Content/images/4.png" alt="4" border="0" id="virtinst.ram" />
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <img class="callout" src="Common_Content/images/5.png" alt="5" border="0" id="virtinst.disk" />
               --cdrom /srv/isos/debian-7.2.0-amd64-netinst.iso  <img class="callout" src="Common_Content/images/6.png" alt="6" border="0" id="virtinst.cdrom" />
               --network bridge=br0      <img class="callout" src="Common_Content/images/7.png" alt="7" border="0" id="virtinst.network" />
               --vnc                     <img class="callout" src="Common_Content/images/8.png" alt="8" border="0" id="virtinst.vnc" />
               --os-type linux           <img class="callout" src="Common_Content/images/9.png" alt="9" border="0" id="virtinst.os" />
               --os-variant debianwheezy
</code></strong><code class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Cannot open display:
Run 'virt-viewer --help' to see a full list of available command line options.
Domain installation still in progress. You can reconnect
to the console to complete the installation process.
</code></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.connect"><img class="callout" src="Common_Content/images/1.png" alt="1" border="0" id="virtinst.connect" /></a> </p></td><td valign="top" align="left"><div class="para">
							Die Option <code class="literal">--connect</code> legt den zu verwendenden „Hypervisor“ fest. Sie hat die Form einer URL, die ein Virtualisierungssystem enthält (<code class="literal">xen://</code>, <code class="literal">qemu://</code>, <code class="literal">lxc://</code>, <code class="literal">openvz://</code>, <code class="literal">vbox://</code> und so weiter) und den Rechner, der den virtuellen Rechner aufnehmen soll (dies kann leer bleiben, falls es sich dabei um den lokalen Host handelt). Zusätzlich hierzu, und im Fall vom QEMU/KVM, kann jeder Benutzer virtuelle Rechner, die mit eingeschränkten Berechtigungen laufen, verwalten, wobei der URL-Pfad es ermöglicht, „System“-Rechner (<code class="literal">/system</code>) von anderen (<code class="literal">/session</code>) zu unterscheiden.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.type"><img class="callout" src="Common_Content/images/2.png" alt="2" border="0" id="virtinst.type" /></a> </p></td><td valign="top" align="left"><div class="para">
							Da KVM auf die gleiche Weise wie QEMU verwaltet wird, kann mit <code class="literal">--virt-type kvm</code> die Verwendung von KVM festgelegt werden, obwohl die URL aussieht, als würde QEMU verwendet.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.name"><img class="callout" src="Common_Content/images/3.png" alt="3" border="0" id="virtinst.name" /></a> </p></td><td valign="top" align="left"><div class="para">
							Die Option <code class="literal">--name</code> legt einen (eindeutigen) Namen für den virtuellen Rechner fest.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.ram"><img class="callout" src="Common_Content/images/4.png" alt="4" border="0" id="virtinst.ram" /></a> </p></td><td valign="top" align="left"><div class="para">
							Die Option <code class="literal">--ram</code> ermöglicht es, die Größe des RAM (in MB) festzulegen, das dem virtuellen Rechner zugeordnet wird.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.disk"><img class="callout" src="Common_Content/images/5.png" alt="5" border="0" id="virtinst.disk" /></a> </p></td><td valign="top" align="left"><div class="para">
							Mit <code class="literal">--disk</code> wird der Ort der Abbild-Datei benannt, die die Festplatte unseres virtuellen Rechners darstellen soll; diese Datei wird, falls sie nicht bereits vorhanden ist, in einer Größe (in GB) erstellt, die mit dem Parameter <code class="literal">size</code> festgelegt wird. Der Parameter <code class="literal">format</code> ermöglicht die Auswahl zwischen mehreren Arten der Speicherung der Abbild-Datei. Das voreingestellte Format (<code class="literal">raw</code>) besteht aus einer einzelnen Datei, die in Größe und Inhalt der Platte entspricht. Wir haben hier ein weiter entwickeltes Format gewählt, das für QEMU spezifisch ist, und bei dem man mit einer kleinen Datei beginnen kann, die nur größer wird, wenn der virtuelle Rechner tatsächlich damit beginnt, Platz zu belegen.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.cdrom"><img class="callout" src="Common_Content/images/6.png" alt="6" border="0" id="virtinst.cdrom" /></a> </p></td><td valign="top" align="left"><div class="para">
							Die Option <code class="literal">--cdrom</code> wird zur Anzeige des Ortes verwendet, an dem sich die optische Platte befindet, die für die Installierung benutzt wird. Der Pfad kann entweder ein lokaler Pfad zu einer ISO-Datei sein, eine URL, von der die Datei bezogen werden kann, oder die Gerätedatei eines physischen CD-ROM-Laufwerks (das heißt <code class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.network"><img class="callout" src="Common_Content/images/7.png" alt="7" border="0" id="virtinst.network" /></a> </p></td><td valign="top" align="left"><div class="para">
							Mit <code class="literal">--network</code> wird festgelegt, wie sich die virtuelle Netzwerkkarte in die Netzwerkkonfiguration des Hosts integriert. Das voreingestellte Verhalten (das in unserem Beispiel ausdrücklich erzwungen wird) besteht darin, sie in eine bereits bestehende Netzwerk-Bridge einzubinden. Falls es eine derartige Bridge nicht gibt, kann der virtuelle Rechner das physische Netzwerk nur über NAT erreichen, das heißt, dass er eine Adresse in einem privaten Teilnetzbereich erhält (192.168.122.0/24).
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.vnc"><img class="callout" src="Common_Content/images/8.png" alt="8" border="0" id="virtinst.vnc" /></a> </p></td><td valign="top" align="left"><div class="para">
							<code class="literal">--vnc</code> gibt an, dass die grafische Konsole unter Verwendung von VNC zur Verfügung gestellt werden soll. Das voreingestellte Verhalten des zugeordneten VNC-Servers besteht darin, nur an der lokalen Schnittstelle auf Eingaben zu warten. Fall der VNC-Client auf einem anderen Host laufen soll, muss zur Herstellung der Verbindung ein SSH-Tunnel eingerichtet werden (siehe <a class="xref" href="sect.remote-login.html#sect.ssh-port-forwarding">Abschnitt 9.2.1.3, „Verschlüsselte Tunnel mit Port-Weiterleitung einrichten“</a>). Alternativ kann <code class="literal">--vnclisten=0.0.0.0</code> verwendet werden, so dass von allen Schnittstellen aus auf den VNC-Server zugegriffen werden kann. Man beachte jedoch, dass in diesem Fall die Firewall wirklich entsprechend eingestellt werden sollte.
						</div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#virtinst.os"><img class="callout" src="Common_Content/images/9.png" alt="9" border="0" id="virtinst.os" /></a> </p></td><td valign="top" align="left"><div class="para">
							Mit den Optionen <code class="literal">--os-type</code> und <code class="literal">--os-variant</code> lassen sich einige Parameter des virtuellen Rechners optimieren in Abhängigkeit von den bekannten Funktionen des Betriebssystems, das hier genannt wird.
						</div></td></tr></table></div><div class="para">
					Jetzt läuft der virtuelle Rechner, und wir müssen uns mit der grafischen Konsole verbinden, um den Installationsprozess fortzusetzen. Falls die bisherigen Schritte in einer grafischen Arbeitsumgebung ausgeführt wurden, sollte diese Verbindung von sich aus starten. Anderenfalls, oder falls wir aus der Ferne arbeiten, kann <code class="command">virt-viewer</code> von jeder beliebigen grafischen Umgebung aus aufgerufen werden, um die grafische Konsole zu öffnen (man beachte, dass zweimal nach dem Root-Passwort des entfernten Hosts gefragt wird, da dieser Arbeitsgang zwei SSH-Verbindungen erfordert):
				</div><pre class="screen"><code class="computeroutput">$ </code><strong class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em class="replaceable"><code>server</code></em>/system testkvm
</code></strong><code class="computeroutput">root@server's password: 
root@server's password: </code></pre><div class="para">
					Wenn der Installationsprozess endet, startet der virtuelle Rechner neu und ist dann einsatzbereit.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785307886064"></a>12.2.3.4. Rechner mit <code class="command">virsh</code> verwalten</h4></div></div></div><a id="idm139785307884784" class="indexterm"></a><div class="para">
					Nachdem die Installation nunmehr erledigt ist, wollen wir sehen, wie man mit den vorhandenen virtuellen Rechnern umgeht. Zunächst soll <code class="command">libvirtd</code> nach einer Liste der virtuellen Rechner, die er verwaltet, gefragt werden:
				</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div class="para">
					Lassen Sie uns unseren virtuellen Testrechner starten:
				</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code class="computeroutput">Domain testkvm started</code></pre><div class="para">
					Wir können jetzt die Verbindungshinweise für die grafische Konsole bekommen (die angegebene VNC-Anzeige kann als Parameter an <code class="command">vncviewer</code> übergeben werden):
				</div><pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code class="computeroutput">:0</code></pre><div class="para">
					Zu den weiteren bei <code class="command">virsh</code> verfügbaren Unterbefehlen gehören:
				</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
							<code class="literal">reboot</code>, um einen virtuellen Rechner neu zu starten;
						</div></li><li class="listitem"><div class="para">
							<code class="literal">shutdown</code>, um ein sauberes Herunterfahren einzuleiten;
						</div></li><li class="listitem"><div class="para">
							<code class="literal">destroy</code>, um ihn brutal zu stoppen;
						</div></li><li class="listitem"><div class="para">
							<code class="literal">suspend</code>, um ihn in den Bereitschaftsbetrieb zu versetzen;
						</div></li><li class="listitem"><div class="para">
							<code class="literal">resume</code>, um ihn wieder in Betrieb zu nehmen;
						</div></li><li class="listitem"><div class="para">
							<code class="literal">autostart</code>, um den automatischen Start des virtuellen Rechners beim Hochfahren des Hosts zu aktivieren (oder ihn mit der Option <code class="literal">--disable</code> zu deaktivieren);
						</div></li><li class="listitem"><div class="para">
							<code class="literal">undefine</code>, um alle Spuren des virtuellen Rechners von <code class="command">libvirtd</code> zu entfernen.
						</div></li></ul></div><div class="para">
					Alle diese Unterbefehle erfordern als einen ihrer Parameter die Kennung eines virtuellen Rechners.
				</div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="idm139785307864368"></a>12.2.3.5. Ein RPM-basiertes System in Debian mit yum installieren</h4></div></div></div><div class="para">
					Wenn auf der virtuellen Maschine ein Debian-System (oder eines seiner Derivate) laufen soll, kann das System mit <code class="command">debootstrap</code> aufgesetzt werden, wie oben beschrieben. Soll aber auf der virtuellen Maschine ein RPM-basiertes System laufen, dann muss es mit dem Utility <code class="command">yum</code> (aus dem gleichnamigen Paket) installiert werden.
				</div><div class="para">
					Bei deisem Vorgehen muss man eine Konfigurationsdatei <code class="filename">yum.conf</code> mit den nötigen Parametern erstellen, einschließlich des Pfades zu den RPM-Repositorien mit den Quellen, dem Pfad zu der Plugin-Konfiguration und dem Ziel-Verzeichnis. Für dieses Beispiel nehmen wir an, dass die Umgebung in <code class="filename">/var/tmp/yum-bootstrap</code> gespeichert ist. Die Datei <code class="filename">/var/tmp/yum-bootstrap/yum.conf</code> sollte folgendermaßen aussehen:
				</div><pre class="programlisting">[main]
reposdir=/var/tmp/yum-bootstrap/repos.d
pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d
cachedir=/var/cache/yum
installroot=/path/to/destination/domU/install
exclude=$exclude
keepcache=1
#debuglevel=4  
#errorlevel=4
pkgpolicy=newest
distroverpkg=centos-release
tolerant=1
exactarch=1
obsoletes=1
gpgcheck=1
plugins=1
metadata_expire=1800</pre><div class="para">
					Das Verzeichnis <code class="filename">/var/tmp/yum-bootstrap/repos.d</code> sollte die Beschreibung der RPM-Quell-Repositorien enthalten, genauso wie in <code class="filename">/etc/yum.repos.d</code> in einem bereits installierten RPM-basierten System. Hier ein Beispiel für ein CentOS 6 installation:
				</div><pre class="programlisting">[base]
name=CentOS-6 - Base
#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os
gpgcheck=1
gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6
   
[updates]
name=CentOS-6 - Updates
#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates
gpgcheck=1
gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6

[extras]
name=CentOS-6 - Extras
#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras
gpgcheck=1
gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6

[centosplus]
name=CentOS-6 - Plus
#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus
gpgcheck=1
gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6
</pre><div class="para">
					Und schlussendlich sollte <code class="filename">pluginconf.d/installonlyn.conf</code> folgendes enthalten:
				</div><pre class="programlisting">[main]
enabled=1
tokeep=5
</pre><div class="para">
					Wenn dann alles aufgesetzt ist, muss sichergestellt werden, dass die <code class="command">rpm</code>-Datenbanken richtig initialisiert sind, z.B. mit einem Befehl wie <code class="command">rpm --rebuilddb</code>. Eine Installation von CentOS 6 ist dann nur noch eine Frage von:
				</div><pre class="screen"><strong class="userinput"><code>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</code></strong></pre></div></div></div><ul class="docnav"><li class="previous"><a accesskey="p" href="advanced-administration.html"><strong>Zurück</strong>Kapitel 12. Erweiterte Verwaltung</a></li><li class="up"><a accesskey="u" href="#"><strong>Nach oben</strong></a></li><li class="home"><a accesskey="h" href="index.html"><strong>Zum Anfang</strong></a></li><li class="next"><a accesskey="n" href="sect.automated-installation.html"><strong>Weiter</strong>12.3. Automatische Installation</a></li></ul></body></html>